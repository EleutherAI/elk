dataset: boolq_es
templates:
  3e386463-1715-4578-9cba-07d11a0d3b61: !Template
    answer_choices: Falso ||| Verdadero
    id: 3e386463-1715-4578-9cba-07d11a0d3b61
    jinja: 'Pasaje: {{passage}}


      Después de leer este pasaje, tengo una pregunta: {{question}}? ¿Verdadero o falso?
      |||


      {% if answer != -1 %}
      {{answer_choices[answer]}}
      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: after_reading
    reference: ''
  492f0f88-4370-46cd-839b-1de37a55aeda: !Template
    answer_choices: No ||| Sí
    id: 492f0f88-4370-46cd-839b-1de37a55aeda
    jinja: "{{ passage }} \nPregunta: {{ question }}\nRespuesta: ||| \n{% if answer !=\
      \ -1 %}\n{{ answer_choices[answer] }}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: GPT-3 Style
    reference: Same as Figure G29, p. 58 of the GPT-3 paper
  6cb6a026-c070-470a-b75d-bb8fdf424e35: !Template
    answer_choices: No ||| Sí
    id: 6cb6a026-c070-470a-b75d-bb8fdf424e35
    jinja: "{{ passage }} \n\nHabiendo leído eso, me pregunto {{ question }}? |||\n{% if\
      \ answer != -1 %}\n{{ answer_choices[answer] }}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: "I wonder\u2026"
    reference: ''
  7cf7acdf-e3a2-459f-a3e8-2e2d27dd6aa5: !Template
    answer_choices: No ||| Sí
    id: 7cf7acdf-e3a2-459f-a3e8-2e2d27dd6aa5
    jinja: 'Texto: {{passage}}


      Responda la siguiente pregunta sí/no: {{question}}? ¿Sí o no? |||


      {% if answer != -1 %}
      {{answer_choices[answer]}}
      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: yes_no_question
    reference: ''
  7d21d974-0624-4d4f-9e8c-644e2d009cb5: !Template
    answer_choices: No ||| Sí
    id: 7d21d974-0624-4d4f-9e8c-644e2d009cb5
    jinja: "{{ passage }} \n\nHabiendo leído eso, ¿podría decirme {{ question }}?\
      \ ||| {% if answer != -1 %}{{ answer_choices[answer] }}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: "could you tell me\u2026"
    reference: ''
  922d3e87-ac58-4731-84d1-f0a40e47afb5: !Template
    answer_choices: No ||| Sí
    id: 922d3e87-ac58-4731-84d1-f0a40e47afb5
    jinja: "EXAMEN\n1. Responda sí o no.\n\nDocumento: {{passage}}\nPregunta: {{question}}?\
      \ ||| \n{% if answer != -1 %}\n{{answer_choices[answer]}}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: exam
    reference: ''
  9a1bf459-8047-437c-9def-f21e960429cc: !Template
    answer_choices: No ||| Sí
    id: 9a1bf459-8047-437c-9def-f21e960429cc
    jinja: "Basado en el siguiente pasaje, {{ question }}?\n\n{{ passage }}\
      \ ||| \n\n{% if answer != -1 %}{{ answer_choices[answer] }}{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: based on the following passage
    reference: "Adapted from Perez et al. 2021 and Schick & Sch\xFCtz 2021."
  9f4c6b0a-437b-40c0-b467-db4b7218d38d: !Template
    answer_choices: Falso ||| Verdadero
    id: 9f4c6b0a-437b-40c0-b467-db4b7218d38d
    jinja: 'Ejercicio: lee el texto y responde la pregunta con Verdadero o Falso.


      Texto: {{passage}}

      Pregunta: {{question}}? |||


      {% if answer != -1 %}
      {{answer_choices[answer]}}
      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: exercise
    reference: ''
  b2b3cb60-d6e3-491c-a09a-8201e13e417e: !Template
    answer_choices: No ||| Sí
    id: b2b3cb60-d6e3-491c-a09a-8201e13e417e
    jinja: '{{ passage }}

      Basado en el pasaje anterior, {{ question }}? |||
      {% if answer != -1 %}
      {{ answer_choices[answer]}}
      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: based on the previous passage
    reference: "Adapted from Perez et al. 2021 and Schick & Sch\xFCtz 2021."
  eb78772c-e81e-4b8a-a77b-b75efd1c212a: !Template
    answer_choices: Falso ||| Verdadero
    id: eb78772c-e81e-4b8a-a77b-b75efd1c212a
    jinja: '{{passage}}


      P: ¿{{question}}? ¿Verdadero o falso? |||


      {% if answer != -1 %}
      {{answer_choices[answer]}}
      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - es
      metrics:
      - Accuracy
      original_task: true
    name: valid_binary
    reference: ''
